# -*- coding: utf-8 -*-
"""Predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OXYXcUG5NBludzSeIV8w7LYEJxoUlIZG
"""

import os
import numpy as np
import pandas as pd
from fitter import Fitter
import torch.utils.data as torch_data
import torchvision
from torchvision import transforms
import torchvision.utils as vutils
import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F

## Helper functions

# z-score normalization
def normalize(data):
  data = torch.FloatTensor(data)
  mean = data.mean()
  std = data.std()
  if std != 0:
      data.add_(-mean)
      data.div_(std)
  return data

def makeDatasets(trainLoader, testLoader, train_batch_size=4, test_batch_size=1):
  # Split loader to train and validation
  train_size = int(len(trainLoader) * 0.9)
  validation_size = len(trainLoader) - train_size
  train_loader, val_loader = torch_data.random_split(trainLoader, [train_size, validation_size])

  # Create train dataset
  train_dataset = torch_data.DataLoader(
        train_loader, batch_size=train_batch_size, shuffle=True,
        num_workers=2, pin_memory=True, sampler=None)

  # Create validation dataset
  val_dataset = torch_data.DataLoader(
        val_loader, batch_size=train_batch_size, shuffle=True,
        num_workers=2, pin_memory=True, sampler=None)
  
  # Create test dataset
  test_dataset = torch_data.DataLoader(
        testLoader, batch_size=test_batch_size, shuffle=False,
        num_workers=2, pin_memory=True, sampler=None)
  
  return train_dataset, val_dataset, test_dataset


## Classes

# data loader class for train data
class DataLoaderClass(torch_data.Dataset):
  def __init__(self, train_data, distributions_opt):
    super(DataLoaderClass, self).__init__()
    self.train_data = train_data
    self.distributions_opt = distributions_opt

  def __getitem__(self, index):
    # extract data and target distribution from given train data
    data, distribution = self.train_data[index]

    # prepare data to use on module    
    data = data.resize_((1, 64))
    data = normalize(data)
    
    # find target distribution index in options 
    label = self.distributions_opt.index(distribution)

    return data, label
  def __len__(self):
    return len(self.train_data)


# data loader class for test data
class TestDataLoaderClass(torch_data.Dataset):
  def __init__(self, test_data):
    super(TestDataLoaderClass, self).__init__()
    self.test_data = test_data

  def __getitem__(self, index):
    # extract data and feature name from given test data
    data, feature_name = self.test_data[index]

    # prepare data to use on module    
    data = data.resize_((1, 64))
    data = normalize(data)

    return data, feature_name
  def __len__(self):
    return len(self.test_data)

# Predict Distribution Model
class PredictDistributionModel(nn.Module):
  def __init__(self, input_size, distributions_size):
    super(PredictDistributionModel, self).__init__()
    self.input_size = input_size
    self.distributions_size = distributions_size
    # conv layer
    self.convLayer1 = nn.Sequential(
        nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(64),
        nn.LeakyReLU())
    # conv layer
    self.convLayer2 = nn.Sequential(
        nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(128),
        nn.LeakyReLU())
    # conv layer
    self.convLayer3 = nn.Sequential(
        nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(256),
        nn.LeakyReLU())
    # conv layer
    self.convLayer4 = nn.Sequential(
        nn.Conv1d(256, 128, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(128),
        nn.LeakyReLU())
    # conv layer
    self.convLayer5 = nn.Sequential(
        nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(),
        nn.Dropout())
            
    self.input_size = self.input_size * 64

    # Fully connected layer
    self.fcLayer = nn.Sequential(
        nn.Linear(self.input_size, self.distributions_size),
        nn.LogSoftmax(dim=1))

  def forward(self, x):
    conv1 = self.convLayer1(x)
    conv2 = self.convLayer2(conv1)    
    conv3 = self.convLayer3(conv2)
    conv4 = self.convLayer4(conv3)
    conv5 = self.convLayer5(conv4)

    conv5 = conv5.view(-1, self.input_size)

    return self.fcLayer(conv5)

# use machine learning to predict distributions
class PredictDistribution:
  def __init__(self, trainData, testData, distributions, lr=0.000001, train_batch_size=4, test_batch_size=1):
    # data
    self.trainData = trainData
    self.testData = testData
    self.distributions = distributions
    self.train_batch_size = train_batch_size
    self.test_batch_size = test_batch_size
    self.lr = lr
    self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # loaders
    self.train_loader = DataLoaderClass(train_data=self.trainData, distributions_opt=self.distributions)
    self.test_loader = TestDataLoaderClass(test_data=self.testData)
    
    # datasets
    self.train_dataset, self.val_dataset, self.test_dataset = makeDatasets(trainLoader=self.train_loader, testLoader=self.test_loader, train_batch_size=self.train_batch_size, test_batch_size=self.test_batch_size)

    # init model and optimizer
    self.predictiveModel = PredictDistributionModel(input_size=64, distributions_size=len(self.distributions)).to(self.device)
    self.optimizerModel = torch.optim.Adam(self.predictiveModel.parameters(), lr = self.lr)
    
    # test's results    
    self.predictions = {}
  
  def private_train(self):
    train_loss = 0
    correct = 0
    self.predictiveModel.train()
    for batch_idx, (data, labels) in enumerate(self.train_dataset):
      # move data to device
      data = data.to(self.device)
      labels = labels.to(self.device)

      self.optimizerModel.zero_grad()

      ### Forward ###
      output = self.predictiveModel(data)

      ### LOSS ###
      loss = F.nll_loss(output, labels)

      ### PREDICTION ###
      train_loss += F.nll_loss(output, labels, reduction='sum').item() # sum up
      prediction = output.max(1, keepdim=True)[1]
      correct += prediction.eq(labels.view_as(prediction)).cpu().sum()

      ### Back-propagation ###
      loss.backward()

      ### Update ###
      self.optimizerModel.step()
    return train_loss / len(self.train_dataset.dataset), correct / len(self.train_dataset.dataset)

  def private_validation(self):
    validation_loss = 0
    correct = 0
    self.predictiveModel.eval()
    for batch_idx, (data, labels) in enumerate(self.val_dataset):
      # move data to device
      data = data.to(self.device)
      labels = labels.to(self.device)

      ### Forward ###
      output = self.predictiveModel(data)

      ### LOSS ###
      validation_loss += F.nll_loss(output, labels, reduction='sum').item()  # sum up

      ### PREDICTION ###
      prediction = output.max(1, keepdim=True)[1]
      correct += prediction.eq(labels.view_as(prediction)).cpu().sum()
  
    return validation_loss / len(self.val_dataset.dataset), correct / len(self.val_dataset.dataset)

  def train(self, EPOCHS=200):
    train_loss = np.array([])
    train_accuracy = np.array([])
    val_loss = np.array([])
    val_accuracy = np.array([])
    epochs = np.array([])

    for epoch in range(EPOCHS):
      epochs = np.append(epochs, epoch)
      ### train ###
      loss, accuracy = self.private_train()
      train_loss = np.append(train_loss, loss)
      train_accuracy = np.append(train_accuracy, accuracy)

      ### validation ###
      loss, accuracy = self.private_validation()
      val_loss = np.append(val_loss, loss)
      val_accuracy = np.append(val_accuracy, accuracy)

    # plot avg loss per epoch
    plt.title(f'avg loss per epoch')
    plt.xlabel('Epochs')
    plt.ylabel('avg loss')
    plt.plot(epochs, train_loss, linestyle = 'dotted', label='training')
    plt.plot(epochs, val_loss, linestyle = 'dotted', label='validation')
    plt.legend()
    plt.show()

    # plot avg accuracy per epoch
    plt.title(f'avg accuracy per epoch')
    plt.xlabel('Epochs')
    plt.ylabel('avg accuracy')
    plt.plot(epochs, train_accuracy, linestyle='dotted', label='training')
    plt.plot(epochs, val_accuracy, linestyle='dotted', label='validation')
    plt.legend()
    plt.show()

  # predict distributions for each feature
  def predict(self):
    self.predictiveModel.eval()
    for batch_idx, (data, feature_name) in enumerate(self.test_dataset):
      # move data to device
      data = data.to(self.device)
      feature_name = feature_name.to(self.device)

      ### Forward ###
      output = self.predictiveModel(data)

      ### PREDICTION ###
      prediction = output.max(1, keepdim=True)[1]

      self.predictions[feature_name] = self.distributions[prediction]
    
    print(f'** Predict Feature Distribution **')
    results = {'Feature': self.predictions.keys(), 'Prediction': self.predictions.values()}
    results = pd.DataFrame(data=results)
    return self.predictions, results
