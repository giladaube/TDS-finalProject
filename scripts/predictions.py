# -*- coding: utf-8 -*-
"""Predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OXYXcUG5NBludzSeIV8w7LYEJxoUlIZG
"""

import os
import numpy as np
import pandas as pd
from fitter import Fitter
import torch.utils.data as torch_data
import torchvision
from torchvision import transforms
import torchvision.utils as vutils
import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F

## Helper functions

# z-score normalization
def normalize(data):
  data = torch.FloatTensor(data)
  mean = data.mean()
  std = data.std()
  if std != 0:
      data.add_(-mean)
      data.div_(std)
  return data

def makeDatasets(data, batch_s):
  # Split dataset to train and validation
  train_size = int(len(data) * 0.9)
  validation_size = len(data) - train_size
  train_dataset, val_dataset = torch_data.random_split(data, [train_size, validation_size])

  # Create train loader
  train_loader = torch_data.DataLoader(
        train_dataset, batch_size=batch_s, shuffle=True,
        num_workers=2, pin_memory=True, sampler=None)

  # Create validation loader
  val_loader = torch_data.DataLoader(
        val_dataset, batch_size=batch_s, shuffle=True,
        num_workers=2, pin_memory=True, sampler=None)

  return train_loader, val_loader


## Classes

# data loader class for train data
class DataLoaderClass(torch_data.Dataset):
  def __init__(self, train_data, distributions_opt):
    super(DataLoaderClass, self).__init__()
    self.train_data = train_data
    self.distributions_opt = distributions_opt

  def __getitem__(self, index):
    # extract data and target distribution from given train data
    data, distribution = self.train_data[index]

    # prepare data to use on module    
    data = data.resize_((1, 64))
    data = normalize(data)
    
    # find target distribution index in options 
    label = self.distributions_opt.index(distribution)

    return data, label
  def __len__(self):
    return len(self.train_data)


# data loader class for test data
class TestDataLoaderClass(torch_data.Dataset):
  def __init__(self, test_data):
    super(TestDataLoaderClass, self).__init__()
    self.test_data = test_data

  def __getitem__(self, index):
    # extract data and target distribution from given train data
    data, label = self.train_data[index]

    # prepare data to use on module    
    data = data.resize_((1, 64))
    data = normalize(data)

    return data, label
  def __len__(self):
    return len(self.test_data)

# Predict Distribution Model
class PredictDistributionModel(nn.Module):
  def __init__(self, input_size, distributions_size):
    super(PredictDistributionModel, self).__init__()
    self.input_size = input_size
    self.distributions_size = distributions_size
    # conv layer
    self.convLayer1 = nn.Sequential(
        nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(64),
        nn.LeakyReLU())
    # conv layer
    self.convLayer2 = nn.Sequential(
        nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(128),
        nn.LeakyReLU())
    # conv layer
    self.convLayer3 = nn.Sequential(
        nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(256),
        nn.LeakyReLU())
    # conv layer
    self.convLayer4 = nn.Sequential(
        nn.Conv1d(256, 128, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm1d(128),
        nn.LeakyReLU())
    # conv layer
    self.convLayer5 = nn.Sequential(
        nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(),
        nn.Dropout())
            
    self.input_size = self.input_size * 64

    # Fully connected layer
    self.fcLayer = nn.Sequential(
        nn.Linear(self.input_size, self.distributions_size),
        nn.LogSoftmax(dim=1))

  def forward(self, x):
    conv1 = self.convLayer1(x)
    conv2 = self.convLayer2(conv1)    
    conv3 = self.convLayer3(conv2)
    conv4 = self.convLayer4(conv3)
    conv5 = self.convLayer5(conv4)

    conv5 = conv5.view(-1, self.input_size)

    return self.fcLayer(conv5)

# use machine learning to predict distributions
class PredictDistribution:
  def __init__(self, data, testData, distributions, lr=0.000001 ,batch_size=1):
    self.data = data
    self.testData = testData
    self.distributions = distributions
    self.t_data = DataLoaderClass(train_data=self.data, distributions_opt=self.distributions)
    self.test_data = TestDataLoaderClass(test_data=self.testData)
    self.batch_size = batch_size
    self.lr = lr
    self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    self.train_loader, self.val_loader =  makeDatasets(self.t_data, self.batch_size)
    self.test_loader =  torch_data.DataLoader(
        self.test_data, batch_size=1, shuffle=False,
        num_workers=2, pin_memory=True, sampler=None)
    self.predictiveModel = PredictDistributionModel(input_size=64, distributions_size=len(self.distributions)).to(self.device)
    self.optimizerModel = torch.optim.Adam(self.predictiveModel.parameters(), lr = self.lr)
    self.predictions = {}
  
  def trainP(self):
    train_loss = 0
    correct = 0
    self.predictiveModel.train()
    for batch_idx, (data, labels) in enumerate(self.train_loader):
      # move data to device
      data = data.to(self.device)
      labels = labels.to(self.device)

      self.optimizerModel.zero_grad()

      ### Forward ###
      output = self.predictiveModel(data)

      ### LOSS ###
      loss = F.nll_loss(output, labels)

      ### PREDICTION ###
      train_loss += F.nll_loss(output, labels, reduction='sum').item() # sum up
      prediction = output.max(1, keepdim=True)[1]
      correct += prediction.eq(labels.view_as(prediction)).cpu().sum()

      ### Back-propagation ###
      loss.backward()

      ### Update ###
      self.optimizerModel.step()
    return train_loss / len(self.train_loader.dataset), correct / len(self.train_loader.dataset)

  def validationP(self):
    validation_loss = 0
    correct = 0
    self.predictiveModel.eval()
    for batch_idx, (data, labels) in enumerate(self.val_loader):
      # move data to device
      data = data.to(self.device)
      labels = labels.to(self.device)

      ### Forward ###
      output = self.predictiveModel(data)
      # print(f'labels: {labels}')
      # print(f'output: {output}')

      ### LOSS ###
      validation_loss += F.nll_loss(output, labels, reduction='sum').item()  # sum up

      ### PREDICTION ###
      prediction = output.max(1, keepdim=True)[1]
      correct += prediction.eq(labels.view_as(prediction)).cpu().sum()
  
    return validation_loss / len(self.val_loader.dataset), correct / len(self.val_loader.dataset)

  def train(self, EPOCHS=200):
    # min_loss = 1000
    train_loss = np.array([])
    train_accuracy = np.array([])
    val_loss = np.array([])
    val_accuracy = np.array([])
    epochs = np.array([])

    for epoch in range(EPOCHS):
      epochs = np.append(epochs, epoch)
      ### train ###
      loss, accuracy = self.trainP()
      train_loss = np.append(train_loss, loss)
      train_accuracy = np.append(train_accuracy, accuracy)

      ### validation ###
      loss, accuracy = self.validationP()
      val_loss = np.append(val_loss, loss)
      val_accuracy = np.append(val_accuracy, accuracy)
      print(f'loss: {loss}, accuracy: {accuracy}')

      # if loss < min_loss:
      #   min_loss = loss
      #   torch.save(model.state_dict(), 'MODEL_PATH')

    # plot avg loss per epoch
    plt.title(f'avg loss per epoch')
    plt.xlabel('Epochs')
    plt.ylabel('avg loss')
    plt.plot(epochs, train_loss, linestyle = 'dotted', label='training')
    plt.plot(epochs, val_loss, linestyle = 'dotted', label='validation')
    plt.legend()
    plt.show()

    # plot avg accuracy per epoch
    plt.title(f'avg accuracy per epoch')
    plt.xlabel('Epochs')
    plt.ylabel('avg accuracy')
    plt.plot(epochs, train_accuracy, linestyle='dotted', label='training')
    plt.plot(epochs, val_accuracy, linestyle='dotted', label='validation')
    plt.legend()
    plt.show()

  # predict distributions for each feature
  def predict(self):
    self.predictiveModel.eval()
    for batch_idx, (data, labels) in enumerate(self.test_loader):
      # move data to device
      data = data.to(self.device)

      ### Forward ###
      output = self.predictiveModel(data)

      ### PREDICTION ###
      prediction = output.max(1, keepdim=True)[1]

      self.predictions[labels] = self.distributions[prediction]
      print(f'{labels}:  prediction={prediction}  ')
    return self.predictions
