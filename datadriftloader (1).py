# -*- coding: utf-8 -*-
"""DataDriftLoader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H7yswCznN4MvMENlBGqGLl8UIZP9N0nH
"""

import numpy as np
import pandas as pd
from fitter import Fitter
from river import drift
import os
import torch

# detect and split given data by drifts
def detect_drifts(data):
  drift_detector = drift.ADWIN()
  drifts = []
  for index, value in enumerate(data):
    # Data is processed one sample at a time
    drift_detector.update(value)
    # The drift detector indicates after each sample if there is a drift in the data
    if drift_detector.change_detected:
      drifts.append(index)
      # As a best practice, we reset the detector
      drift_detector.reset()
  return drifts

# find best distribution to given data, from a list of distributions
def findOutDistribution(data, distributions_opt):
  f = Fitter(data, timeout=30, distributions=distributions_opt)
  f.fit()
  return f.get_best(method = 'sumsquare_error')

class DataDriftLoader:
  def __init__(self, data_path, distributions):
    self.data_path = data_path
    self.dataset = pd.read_csv(self.data_path)
    self.distributions = distributions
    self.drifts = {}
    self.before = None
    self.after = None
    self.trainData = []
    self.testData = []
    
  def removeColumns(self, l_columns):
    self.dataset.drop(columns=l_columns, inplace=True)

  def findDrifts(self):
    for feature_name in self.dataset.columns:
      feature_values = self.dataset[feature_name].to_numpy()
      # detect drifts
      feature_drifts = detect_drifts(feature_values)
      self.drifts[feature_name] = feature_drifts

  # split data to before&after latest main drift
  def splitForFTB(self):
    index = len(self.dataset)
    # find out the last drift for all features
    for feature_name in self.dataset.columns:
      feature_index = self.drifts.get(feature_name)[-1]
      if feature_index < index:
        index = feature_index
    
    # split data where it's the first drift from latest drifts of all features
    before = self.dataset.iloc[:, :index]
    after = self.dataset.iloc[index:, :]

    self.before = before
    self.after = after

    # return data before and after drift
    return before, after

  # split data by features and drifts
  def splitForPrediction(self):
    for feature_name in self.before.columns:
      feature_values = self.before[feature_name].to_numpy()
      # detect drifts
      drifts = detect_drifts(feature_values) 
      # split the feature by distribution
      feature_splited = np.split(feature_values, drifts)

      # create train data of data chunk and distribution after the drift
      for index in range(len(feature_splited) - 1):
        drift_data = pd.DataFrame(feature_splited[index], columns=[feature_name])
        distribution, values = findOutDistribution(feature_splited[index + 1], self.distributions).popitem()
        # add to train list
        data_ = torch.tensor(drift_data.astype(np.float32).values)
        data_ = torch.reshape(data_, (1, len(data_)))
        self.trainData.append([data_, distribution])
    # return list of data with labels
    return self.trainData

  def getTestData(self):
    for feature_name in self.dataset.columns:
      feature_values = self.dataset[feature_name].to_numpy()

      # add to test data list
      data_ = torch.tensor(feature_values.astype(np.float32).values)
      data_ = torch.reshape(data_, (1, len(data_)))
      self.testData.append([data_, feature_name])
    # return list of data with labels
    return self.testData